# ğŸ—ºï¸ Build Roadmap

El proyecto evoluciona mediante hitos incrementales y reproducibles.  
Cada paso aÃ±ade una capacidad real de producciÃ³n.

---

## ğŸ“Œ Resumen de los 8 pasos

1. **Project setup, environment & CLI foundation**  
2. **Source adapter (GitHub Pages catalog)**  
3. **Historical persistence (CSV/JSON pipeline)**  
4. **Download & cache product images locally**  
5. **HTML dashboard with price timeline (Chart.js)**  
6. **Automated tests (normalization & dedupe)**  
7. **Dockerized execution**  
8. **Automation: local dev loop + GitHub Actions scheduled runs**

Ahora bajamos al detalle ğŸ‘‡

---

# 1 â€” Project Setup, Environment & CLI Foundation  
**Issue:** #1â€“#2

### ğŸ¯ Goal  
Inicializar un proyecto Python profesional usando uv, definir la estructura y asegurar que el CLI funciona.

### â–¶ï¸ Run  
```bash
uv run python -m scraper.cli healthcheck
```

### ğŸ“¦ What was introduced
- project layout  
- dependency management  
- minimal CLI  
- reproducible environment  

---

# 2 â€” Source Adapter (GitHub Pages Catalog)  
**Issue:** #3

### ğŸ¯ Goal  
Scrapear pÃ¡ginas controladas y devolver snapshots estructurados.

**Source:**  
ğŸ‘‰ https://andres-torrez.github.io/iphone-catalog/

### â–¶ï¸ Run  
```bash
uv run python -m scraper.cli scrape
```

### ğŸ“¤ Output  
JSON con:
- model  
- title  
- sku  
- price  
- image  
- timestamp  

### ğŸ“¦ What was introduced
- adapter pattern  
- HTML parsing  
- price normalization  
- typed data model  

---

# 3 â€” Historical Persistence (CSV/JSON Pipeline)  
**Issue:** #4

### ğŸ¯ Goal  
Convertir snapshots en un dataset histÃ³rico.

### â–¶ï¸ Run  
```bash
uv run python -m scraper.cli run
```

### ğŸ“¤ Output  
```
data/processed/prices.json
data/processed/prices.csv
```

### ğŸ“¦ What was introduced
- history merge  
- deduplication  
- reproducible exports  
- storage layer  

Ahora el proyecto se comporta como un **data pipeline**, no un script.

---

# 4 â€” Download & Cache Product Images Locally  
**Issue:** #5

### ğŸ¯ Goal  
Hacer los reportes independientes de internet.

### â–¶ï¸ Run  
```bash
uv run python -m scraper.cli run
```

### ğŸ“¤ Output  
```
assets/images/
```

Y `image_path` dentro del dataset.

### ğŸ“¦ What was introduced
- media pipeline  
- cache strategy  
- deterministic assets  

---

# 5 â€” Generate the HTML Dashboard (Chart.js)  
**Issue:** #6

### ğŸ¯ Goal  
Convertir el histÃ³rico en un producto visual.

### â–¶ï¸ Run  
```bash
uv run python -m scraper.cli run
```

### ğŸ“¤ Output  
```
reports/index.html
```

### ğŸ“Š Dashboard includes
- latest price per model  
- delta vs previous  
- timeline graph  
- cached images  
- last update timestamp  

Ahora stakeholders pueden ver el sistema funcionando.

---

# 6 â€” Automated Tests (Normalization & Dedupe)  
**Issue:** #7

### ğŸ¯ Goal  
Garantizar la correcciÃ³n de los datos.

### â–¶ï¸ Run  
```bash
uv run pytest -q
```

### ğŸ§ª Tests cover
- European price parsing  
- duplicate detection  

### ğŸ“¦ What was introduced
- repeatability  
- trust in the pipeline  
- CI readiness  

---

# 7 â€” Dockerized Execution  
**Issue:** #8

### ğŸ¯ Goal  
Ejecutar todo con un solo comando, en cualquier sitio.

### â–¶ï¸ Build  
```bash
docker build -t iphone-monitor .
```

### â–¶ï¸ Run  
```bash
docker run --rm -v "$(pwd):/app" iphone-monitor
```

### ğŸ“¤ Output  
Los mismos artefactos:
- CSV  
- JSON  
- images  
- HTML report  

Ahora el proyecto es **portable**.

---

# 8 â€” Automation: Local Dev Loop + GitHub Actions  
**Issue:** #9

### ğŸ¯ Goal  
Eliminar la ejecuciÃ³n manual.

---

## ğŸ–¥ï¸ Local development loop (visual)

Corre cada 2 minutos:

- tests  
- scraper  
- report generation  

```powershell
powershell -ExecutionPolicy Bypass -File scripts/dev_loop.ps1
```

Puedes ver los archivos actualizÃ¡ndose en VS Code.

---

## â˜ï¸ GitHub Actions schedule

Corre automÃ¡ticamente desde `main`.

Pipeline:

- install  
- pytest  
- run scraper  
- upload artifacts  

TambiÃ©n puedes lanzarlo manualmente desde **Actions â†’ Run workflow**.

---

# ğŸ‰ Final System Capability

Al terminar el paso 8 tienes:

- modular architecture  
- real scraping  
- history  
- visual dashboard  
- local assets  
- tests  
- docker  
- automation  
- reproducibility  
- portfolio-level engineering  
